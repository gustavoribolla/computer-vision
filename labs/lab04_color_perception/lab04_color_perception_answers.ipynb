{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 4: Color Perception â€” ğŸ“ ANSWERS\n",
    "\n",
    "### â±ï¸ Time Guide (90 min)\n",
    "\n",
    "| Section | Time | Notes |\n",
    "|---------|------|-------|\n",
    "| Setup + Image loading | 5 min | Help with import issues |\n",
    "| Part 1: BGR segmentation | 10 min | Show the problem |\n",
    "| Part 2: HSV (Activity 1) | 20 min | Most important section |\n",
    "| Part 3: HLS (Activity 2) | 15 min | Quick pattern recognition |\n",
    "| Part 4: Lab (Activity 3) | 15 min | Similar to HSV/HLS |\n",
    "| Part 5: Grayscale (Challenges) | 20 min | Debugging + research |\n",
    "| Reflection + wrap-up | 5 min | Class discussion |\n",
    "\n",
    "---\n",
    "\n",
    "### ğŸ¯ Learning Goals\n",
    "\n",
    "1. Students understand that BGR is not always the best choice\n",
    "2. Students can convert between color spaces using OpenCV\n",
    "3. Students understand when to use HSV vs Lab vs others\n",
    "4. Students learn to read documentation (normalization activities)\n",
    "5. Students experience the debugging process (Challenge 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 4: Color Perception and Color Spaces\n",
    "\n",
    "**Computer Vision Course**\n",
    "\n",
    "In this lab you will explore how different color spaces (RGB, HSV, HLS, Lab) affect image processing tasks. You'll see why simply working with RGB values is often not enough, and how choosing the right color space can make segmentation tasks much easier.\n",
    "\n",
    "**What you'll do:**\n",
    "- Segment the sky from an image using different color spaces\n",
    "- Understand HSV, HLS, and Lab color representations\n",
    "- Learn when each color space is useful\n",
    "- Debug common color conversion issues\n",
    "\n",
    "**Connection to previous labs:**\n",
    "- Lab 3 showed how image variations break models\n",
    "- Today you'll learn about one critical type of variation: **color**\n",
    "- Understanding color spaces is essential for robust computer vision"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Computer Vision Course - Lab 4: Color Perception\n",
    "\n",
    "This cell sets up the environment.\n",
    "Works automatically for both local and Google Colab!\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Detect environment\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"Computer Vision - Lab 4: Color Perception\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "if IN_COLAB:\n",
    "    print(\"\\nğŸ”µ Running on Google Colab\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    if not os.path.exists('computer-vision'):\n",
    "        print(\"ğŸ“¥ Cloning repository...\")\n",
    "        !git clone https://github.com/mjck/computer-vision.git\n",
    "        print(\"âœ“ Repository cloned successfully\")\n",
    "    else:\n",
    "        !git -C computer-vision pull\n",
    "        print(\"âœ“ Repository updated\")\n",
    "    \n",
    "    %cd computer-vision/labs/lab04_color_perception\n",
    "    print(f\"âœ“ Current directory: {os.getcwd()}\")\n",
    "    \n",
    "    sys.path.insert(0, '/content/computer-vision')\n",
    "    print(\"âœ“ Python path configured\")\n",
    "    \n",
    "    print(\"-\" * 60)\n",
    "    print(\"ğŸŸ¢ Colab setup complete!\\n\")\n",
    "    \n",
    "else:\n",
    "    print(\"\\nğŸŸ¢ Running locally\")\n",
    "    print(\"-\" * 60)\n",
    "    print(f\"âœ“ Current directory: {os.getcwd()}\")\n",
    "    \n",
    "    repo_root = os.path.abspath('../..')\n",
    "    if repo_root not in sys.path:\n",
    "        sys.path.insert(0, repo_root)\n",
    "    print(f\"âœ“ Repository root: {repo_root}\")\n",
    "    \n",
    "    print(\"-\" * 60)\n",
    "    print(\"ğŸŸ¢ Local setup complete!\\n\")\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"âœ… Environment ready!\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Import course utilities\n",
    "try:\n",
    "    from sdx import cv_imread, cv_imshow\n",
    "    print(\"âœ“ sdx module loaded\")\n",
    "except ImportError as e:\n",
    "    print(f\"âŒ Could not import sdx: {e}\")\n",
    "    print(\"\\nTroubleshooting:\")\n",
    "    print(\"  1. Check that sdx.py is in repository root\")\n",
    "    print(\"  2. Verify sys.path includes repository root\")\n",
    "    print(f\"  3. Current sys.path: {sys.path[:3]}\")\n",
    "    raise\n",
    "\n",
    "print(\"âœ“ All imports successful\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading and Displaying the Image\n",
    "\n",
    "We'll use a panoramic image of Insper. This time we'll work with the **full color** image, not grayscale."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## read color image and do not convert to RGB\n",
    "image = cv_imread('insper.png', as_rgb=False)\n",
    "\n",
    "rgb_image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "cv_imshow(rgb_image)\n",
    "\n",
    "print(f\"Image loaded: {image.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This image is a three-dimensional array. The third dimension represents the **color channels**. OpenCV stores images in **BGR order** (Blue, Green, Red) instead of RGB, for [historical reasons](https://learnopencv.com/why-does-opencv-use-bgr-color-format/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "height, width, channels = image.shape\n",
    "\n",
    "print(f\"Dimensions: {height} Ã— {width} Ã— {channels}\")\n",
    "print(f\"Data type: {image.dtype}\")\n",
    "print(f\"Value range: [{image.min()}, {image.max()}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 1 â€” Segmenting the Sky in BGR\n",
    "\n",
    "**Task:** Try to identify all pixels that belong to the sky.\n",
    "\n",
    "**Approach:** We'll look for pixels close to cyan (the sky color) using Euclidean distance in BGR color space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the Target Color\n",
    "\n",
    "In BGR, cyan is `(255, 255, 0)` â€” high blue, high green, zero red."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cyan = np.array([255, 255, 0])  # BGR format\n",
    "\n",
    "print(f\"Target color (BGR): {cyan}\")\n",
    "print(f\"This is cyan: high blue + high green, no red\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distance Function\n",
    "\n",
    "We'll measure how close each pixel is to cyan using Euclidean distance. We normalize by 255 so distances are in the range [0, âˆš3]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distance_bgr(pixel, target):\n",
    "    \"\"\"\n",
    "    Compute normalized Euclidean distance between two BGR pixels.\n",
    "    \n",
    "    Args:\n",
    "        pixel: BGR pixel as (B, G, R) in [0, 255]\n",
    "        target: target BGR color as (B, G, R) in [0, 255]\n",
    "    \n",
    "    Returns:\n",
    "        Distance in [0, sqrt(3)] (normalized to [0, 1] range per channel)\n",
    "    \"\"\"\n",
    "    return np.linalg.norm((pixel - target) / 255.0)\n",
    "\n",
    "# Test it\n",
    "test_pixel = np.array([200, 220, 50])  # Sky-ish color\n",
    "print(f\"Distance from cyan to {test_pixel}: {distance_bgr(test_pixel, cyan):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Attempt 1: Segment Using BGR Distance\n",
    "\n",
    "Let's try a threshold of 1.0 â€” any pixel within distance 1.0 of cyan is considered \"sky\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "threshold = 1.0\n",
    "\n",
    "# Create binary mask\n",
    "output = np.zeros((height, width), dtype=np.uint8)\n",
    "\n",
    "for y in range(height):\n",
    "    for x in range(width):\n",
    "        if distance_bgr(image[y, x], cyan) < threshold:\n",
    "            output[y, x] = 255\n",
    "\n",
    "cv_imshow(output)\n",
    "print(f\"Threshold: {threshold}\")\n",
    "print(f\"Sky pixels detected: {np.sum(output == 255):,} / {height * width:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ¤” Observation\n",
    "\n",
    "That's not very good. We either:\n",
    "- Select too many pixels (false positives â€” grass, buildings)\n",
    "- Select too few pixels (false negatives â€” parts of sky missing)\n",
    "\n",
    "**Why?** BGR is not perceptually uniform. Similar-looking colors can be far apart in BGR space, and different-looking colors can be close together.\n",
    "\n",
    "Let's try other color spaces."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 2 â€” Using the HSV Color Space\n",
    "\n",
    "**HSV = Hue, Saturation, Value**\n",
    "\n",
    "- **Hue:** The color type (red, green, blue, etc.) â€” represented as an angle [0Â°, 360Â°]\n",
    "- **Saturation:** How pure/vivid the color is [0, 1] â€” low = grayish, high = vivid\n",
    "- **Value:** How bright the color is [0, 1] â€” low = dark, high = bright\n",
    "\n",
    "HSV separates *what color* (hue) from *how vivid* (saturation) and *how bright* (value). This often makes color-based segmentation easier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert to HSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n",
    "\n",
    "print(f\"HSV image shape: {hsv.shape}\")\n",
    "print(f\"HSV dtype: {hsv.dtype}\")\n",
    "print(f\"Sample pixel (BGR): {image[100, 200]}\")\n",
    "print(f\"Sample pixel (HSV): {hsv[100, 200]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "âš ï¸ **Important:** Don't use `cv_imshow()` on the HSV image directly! It will look weird because `cv_imshow()` expects BGR format. The HSV values are correct, just not meant for direct visualization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### âœï¸ Activity 1 â€” Normalize HSV Values\n",
    "\n",
    "OpenCV stores HSV in a specific range. Read the [BGR to HSV documentation](https://docs.opencv.org/4.x/de/d25/imgproc_color_conversions.html#color_convert_rgb_hsv) and write a function that normalizes HSV pixels to standard ranges:\n",
    "\n",
    "- **H:** [0, 360] degrees\n",
    "- **S:** [0, 1]\n",
    "- **V:** [0, 1]\n",
    "\n",
    "**Hint:** OpenCV uses 8-bit storage, so values are scaled to fit in [0, 255]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_hsv(pixel):\n",
    "    \"\"\"\n",
    "    Normalize HSV pixel from OpenCV's storage format to standard ranges.\n",
    "    \n",
    "    Args:\n",
    "        pixel: HSV pixel from OpenCV (H, S, V) in OpenCV's range\n",
    "    \n",
    "    Returns:\n",
    "        (h, s, v) tuple where:\n",
    "            h: float in [0, 360] (degrees)\n",
    "            s: float in [0, 1]\n",
    "            v: float in [0, 1]\n",
    "    \n",
    "    TODO: Read OpenCV documentation and implement this function.\n",
    "    Hint: OpenCV stores H in [0, 180], S in [0, 255], V in [0, 255]\n",
    "    \"\"\"\n",
    "    h, s, v = pixel\n",
    "    \n",
    "    # â”€â”€ Your code here â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    return h, s, v  # Replace with correct normalization\n",
    "    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "# Test your function\n",
    "test_hsv = hsv[100, 200]\n",
    "h, s, v = normalize_hsv(test_hsv)\n",
    "print(f\"\\nTest pixel (OpenCV format): {test_hsv}\")\n",
    "print(f\"Normalized: H={h:.1f}Â°, S={s:.3f}, V={v:.3f}\")\n",
    "print(\"\\nâœ“ If H is in [0, 360], S in [0, 1], V in [0, 1], you're correct!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## ğŸ”‘ SOLUTION â€” Activity 1: normalize_hsv()\n",
    "\n",
    "### The Answer\n",
    "\n",
    "```python\n",
    "def normalize_hsv(pixel):\n",
    "    h, s, v = pixel\n",
    "    h = h * 2.0        # OpenCV stores H in [0, 180], we want [0, 360]\n",
    "    s = s / 255.0      # OpenCV stores S in [0, 255], we want [0, 1]\n",
    "    v = v / 255.0      # OpenCV stores V in [0, 255], we want [0, 1]\n",
    "    return h, s, v\n",
    "```\n",
    "\n",
    "### Why OpenCV Uses These Ranges\n",
    "\n",
    "- **H in [0, 180]:** To fit in a single byte (uint8). Hue is circular, so 180 is enough resolution.\n",
    "- **S and V in [0, 255]:** Standard 8-bit range for storage efficiency.\n",
    "\n",
    "### Common Student Mistakes\n",
    "\n",
    "1. **Forgetting to multiply H by 2** â†’ H stays in [0, 180]\n",
    "2. **Dividing H by 180** â†’ Gives [0, 1] instead of [0, 360]\n",
    "3. **Not dividing S and V** â†’ Values stay in [0, 255]\n",
    "\n",
    "### Good HSV Thresholds for Sky\n",
    "\n",
    "```python\n",
    "if 160 < h < 220 and s > 0.2 and v > 0.3:\n",
    "    output[y, x] = 255\n",
    "```\n",
    "\n",
    "These values isolate cyan/blue colors that are reasonably saturated and bright.\n",
    "\n",
    "### Teaching Tip\n",
    "\n",
    "Walk through the HSV segmentation together as a class for the first iteration.\n",
    "Show Google's color picker and how HSV values correspond to perceived colors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Segment Using HSV\n",
    "\n",
    "Now try to segment the sky using HSV thresholds. The sky is typically:\n",
    "- **Hue:** Around 180-220Â° (cyan/blue range)\n",
    "- **Saturation:** Medium to high (0.3-1.0) â€” it's a vivid color\n",
    "- **Value:** Medium to high (0.4-1.0) â€” it's bright\n",
    "\n",
    "Use Google's [color picker](https://www.google.com/search?q=color+picker) to experiment with HSV values and find good thresholds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = np.zeros((height, width), dtype=np.uint8)\n",
    "\n",
    "for y in range(height):\n",
    "    for x in range(width):\n",
    "        h, s, v = normalize_hsv(hsv[y, x])\n",
    "        \n",
    "        # â”€â”€ Your thresholds here â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "        # Replace this trivial condition with one based on h, s, v\n",
    "        # Example: if 180 < h < 220 and s > 0.3 and v > 0.4:\n",
    "        if True:  # TODO: Replace this!\n",
    "        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "            output[y, x] = 255\n",
    "\n",
    "cv_imshow(output)\n",
    "print(f\"Sky pixels detected: {np.sum(output == 255):,} / {height * width:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Goal:** Try to isolate just the sky. Experiment with different threshold ranges!\n",
    "\n",
    "**Tips:**\n",
    "- If you get too many false positives (grass, buildings), make your ranges tighter\n",
    "- If you get too many false negatives (missing sky), make your ranges wider\n",
    "- Hue is circular (0Â° and 360Â° are the same color â€” red)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 3 â€” Using the HLS Color Space\n",
    "\n",
    "**HLS = Hue, Lightness, Saturation**\n",
    "\n",
    "Similar to HSV, but uses **Lightness** instead of Value:\n",
    "- **Hue:** Same as HSV [0Â°, 360Â°]\n",
    "- **Lightness:** How light/dark [0, 1] â€” 0 = black, 0.5 = pure color, 1 = white\n",
    "- **Saturation:** How pure the color is [0, 1]\n",
    "\n",
    "HLS is sometimes better for tasks involving lighting variations.\n",
    "\n",
    "âš ï¸ **Note:** OpenCV calls it \"HLS\" not \"HSL\" (different ordering of letters)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "hls = cv2.cvtColor(image, cv2.COLOR_BGR2HLS)\n",
    "\n",
    "print(f\"HLS image shape: {hls.shape}\")\n",
    "print(f\"Sample pixel (BGR): {image[100, 200]}\")\n",
    "print(f\"Sample pixel (HLS): {hls[100, 200]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### âœï¸ Activity 2 â€” Normalize HLS Values\n",
    "\n",
    "Read the [BGR to HLS documentation](https://docs.opencv.org/4.x/de/d25/imgproc_color_conversions.html#color_convert_rgb_hls) and write a normalization function for HLS:\n",
    "\n",
    "- **H:** [0, 360] degrees\n",
    "- **L:** [0, 1]\n",
    "- **S:** [0, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_hls(pixel):\n",
    "    \"\"\"\n",
    "    Normalize HLS pixel from OpenCV's storage format to standard ranges.\n",
    "    \n",
    "    Args:\n",
    "        pixel: HLS pixel from OpenCV (H, L, S) in OpenCV's range\n",
    "    \n",
    "    Returns:\n",
    "        (h, l, s) tuple where:\n",
    "            h: float in [0, 360] (degrees)\n",
    "            l: float in [0, 1]\n",
    "            s: float in [0, 1]\n",
    "    \n",
    "    TODO: Implement this based on OpenCV documentation.\n",
    "    \"\"\"\n",
    "    h, l, s = pixel\n",
    "    \n",
    "    # â”€â”€ Your code here â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    return h, l, s  # Replace with correct normalization\n",
    "    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "# Test\n",
    "test_hls = hls[100, 200]\n",
    "h, l, s = normalize_hls(test_hls)\n",
    "print(f\"\\nTest pixel (OpenCV format): {test_hls}\")\n",
    "print(f\"Normalized: H={h:.1f}Â°, L={l:.3f}, S={s:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## ğŸ”‘ SOLUTION â€” Activity 2: normalize_hls()\n",
    "\n",
    "### The Answer\n",
    "\n",
    "```python\n",
    "def normalize_hls(pixel):\n",
    "    h, l, s = pixel\n",
    "    h = h * 2.0        # H in [0, 180] â†’ [0, 360]\n",
    "    l = l / 255.0      # L in [0, 255] â†’ [0, 1]\n",
    "    s = s / 255.0      # S in [0, 255] â†’ [0, 1]\n",
    "    return h, l, s\n",
    "```\n",
    "\n",
    "### HLS vs HSV\n",
    "\n",
    "- **Lightness (HLS):** 0 = black, 0.5 = pure color, 1 = white\n",
    "- **Value (HSV):** 0 = black, 1 = pure color at full brightness\n",
    "\n",
    "HLS can be better when dealing with both very dark and very bright regions.\n",
    "\n",
    "### Good HLS Thresholds for Sky\n",
    "\n",
    "```python\n",
    "if 160 < h < 220 and 0.4 < l < 0.9 and s > 0.2:\n",
    "    output[y, x] = 255\n",
    "```\n",
    "\n",
    "### Teaching Tip\n",
    "\n",
    "Have students compare their HSV and HLS results side-by-side.\n",
    "Ask: \"Which worked better for this image? Why?\" (No right answer â€” both can work!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Segment Using HLS\n",
    "\n",
    "Now try segmenting the sky using HLS thresholds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = np.zeros((height, width), dtype=np.uint8)\n",
    "\n",
    "for y in range(height):\n",
    "    for x in range(width):\n",
    "        h, l, s = normalize_hls(hls[y, x])\n",
    "        \n",
    "        # â”€â”€ Your thresholds here â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "        if True:  # TODO: Replace with your condition\n",
    "        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "            output[y, x] = 255\n",
    "\n",
    "cv_imshow(output)\n",
    "print(f\"Sky pixels detected: {np.sum(output == 255):,} / {height * width:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question to think about:** Does HLS work better than HSV for this image? Why or why not?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 4 â€” Using the Lab Color Space\n",
    "\n",
    "**Lab = Lightness, a, b**\n",
    "\n",
    "Lab is designed to be **perceptually uniform** â€” equal distances in Lab space correspond to equal perceived color differences.\n",
    "\n",
    "- **L:** Lightness [0, 100]\n",
    "- **a:** Green (-) to Red (+) axis\n",
    "- **b:** Blue (-) to Yellow (+) axis\n",
    "\n",
    "Lab is often used in color science and can be better for color matching tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lab = cv2.cvtColor(image, cv2.COLOR_BGR2Lab)\n",
    "\n",
    "print(f\"Lab image shape: {lab.shape}\")\n",
    "print(f\"Sample pixel (BGR): {image[100, 200]}\")\n",
    "print(f\"Sample pixel (Lab): {lab[100, 200]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### âœï¸ Activity 3 â€” Normalize Lab Values\n",
    "\n",
    "Read the [BGR to Lab documentation](https://docs.opencv.org/4.x/de/d25/imgproc_color_conversions.html#color_convert_rgb_lab) and normalize Lab values:\n",
    "\n",
    "- **L:** [0, 100]\n",
    "- **a:** [-127, 127] (green to red)\n",
    "- **b:** [-127, 127] (blue to yellow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_lab(pixel):\n",
    "    \"\"\"\n",
    "    Normalize Lab pixel from OpenCV's storage format to standard ranges.\n",
    "    \n",
    "    Args:\n",
    "        pixel: Lab pixel from OpenCV (L, a, b) in OpenCV's range\n",
    "    \n",
    "    Returns:\n",
    "        (L, a, b) tuple where:\n",
    "            L: float in [0, 100]\n",
    "            a: float in [-127, 127]\n",
    "            b: float in [-127, 127]\n",
    "    \n",
    "    TODO: Implement this. Careful with the a and b channels!\n",
    "    \"\"\"\n",
    "    L, a, b = pixel\n",
    "    \n",
    "    # â”€â”€ Your code here â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "    return L, a, b  # Replace with correct normalization\n",
    "    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "# Test\n",
    "test_lab = lab[100, 200]\n",
    "L, a, b = normalize_lab(test_lab)\n",
    "print(f\"\\nTest pixel (OpenCV format): {test_lab}\")\n",
    "print(f\"Normalized: L={L:.1f}, a={a:.1f}, b={b:.1f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## ğŸ”‘ SOLUTION â€” Activity 3: normalize_lab()\n",
    "\n",
    "### The Answer\n",
    "\n",
    "```python\n",
    "def normalize_lab(pixel):\n",
    "    L, a, b = pixel\n",
    "    L = L * (100.0 / 255.0)   # L in [0, 255] â†’ [0, 100]\n",
    "    a = a - 128               # a in [0, 255] â†’ [-128, 127], centered at 128\n",
    "    b = b - 128               # b in [0, 255] â†’ [-128, 127], centered at 128\n",
    "    return L, a, b\n",
    "```\n",
    "\n",
    "### Understanding Lab Space\n",
    "\n",
    "- **L = 0:** Pure black\n",
    "- **L = 100:** Pure white\n",
    "- **a < 0:** Green, **a > 0:** Red\n",
    "- **b < 0:** Blue, **b > 0:** Yellow\n",
    "\n",
    "### Good Lab Thresholds for Sky\n",
    "\n",
    "```python\n",
    "if L > 40 and -40 < a < 10 and -60 < b < -10:\n",
    "    output[y, x] = 255\n",
    "```\n",
    "\n",
    "Sky is bright (high L), slightly green (negative a), and definitely blue (negative b).\n",
    "\n",
    "### Teaching Tip\n",
    "\n",
    "Lab is the most \"scientific\" color space â€” emphasize its perceptual uniformity.\n",
    "Good for color matching tasks in industry."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Segment Using Lab\n",
    "\n",
    "Try segmenting the sky in Lab space. \n",
    "\n",
    "**Hints for sky:**\n",
    "- L: Medium to high (bright)\n",
    "- a: Negative or near zero (no red, some green)\n",
    "- b: Negative (blue, not yellow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output = np.zeros((height, width), dtype=np.uint8)\n",
    "\n",
    "for y in range(height):\n",
    "    for x in range(width):\n",
    "        L, a, b = normalize_lab(lab[y, x])\n",
    "        \n",
    "        # â”€â”€ Your thresholds here â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "        if True:  # TODO: Replace with your condition\n",
    "        # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "            output[y, x] = 255\n",
    "\n",
    "cv_imshow(output)\n",
    "print(f\"Sky pixels detected: {np.sum(output == 255):,} / {height * width:,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Part 5 â€” Converting to Grayscale\n",
    "\n",
    "Finally, let's explore how to properly convert a color image to grayscale."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting Color Channels\n",
    "\n",
    "First, let's split the BGR image into its three channels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "b, g, r = cv2.split(image)\n",
    "\n",
    "# Display each channel\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15, 5))\n",
    "\n",
    "axes[0].imshow(b, cmap='gray', vmin=0, vmax=255)\n",
    "axes[0].set_title('Blue channel')\n",
    "axes[0].axis('off')\n",
    "\n",
    "axes[1].imshow(g, cmap='gray', vmin=0, vmax=255)\n",
    "axes[1].set_title('Green channel')\n",
    "axes[1].axis('off')\n",
    "\n",
    "axes[2].imshow(r, cmap='gray', vmin=0, vmax=255)\n",
    "axes[2].set_title('Red channel')\n",
    "axes[2].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"Each channel shape: {b.shape}\")\n",
    "print(f\"Each channel dtype: {b.dtype}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive Approach: Average the Channels\n",
    "\n",
    "The simplest approach is to average the three channels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Naive grayscale conversion\n",
    "gray = (r + g + b) / 3\n",
    "\n",
    "cv_imshow(gray)\n",
    "\n",
    "print(f\"Gray image dtype: {gray.dtype}\")\n",
    "print(f\"Gray image range: [{gray.min():.1f}, {gray.max():.1f}]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ› Challenge 1 â€” Fix the Bug\n",
    "\n",
    "Something is wrong! The image has weird artifacts. \n",
    "\n",
    "**Your task:**\n",
    "1. Figure out what went wrong\n",
    "2. Fix the code below\n",
    "3. Explain why the original code failed\n",
    "\n",
    "**Hint:** Check the data types involved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Fix this code â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "gray = (r + g + b) / 3  # What's wrong here?\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "cv_imshow(gray)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## ğŸ”‘ SOLUTION â€” Challenge 1: Fix the Grayscale Bug\n",
    "\n",
    "### The Problem\n",
    "\n",
    "The original code caused **integer overflow**:\n",
    "\n",
    "```python\n",
    "gray = (r + g + b) / 3  # WRONG!\n",
    "```\n",
    "\n",
    "When `r`, `g`, and `b` are `uint8` arrays:\n",
    "- Adding them can produce values > 255\n",
    "- But uint8 wraps around: 200 + 100 = 44 (wraps at 256)\n",
    "- This creates weird artifacts\n",
    "\n",
    "### The Fix\n",
    "\n",
    "Convert to a larger data type before arithmetic:\n",
    "\n",
    "```python\n",
    "gray = (r.astype(float) + g.astype(float) + b.astype(float)) / 3\n",
    "# or\n",
    "gray = (r / 3.0 + g / 3.0 + b / 3.0)\n",
    "# or\n",
    "gray = (r.astype(np.float32) + g + b) / 3\n",
    "```\n",
    "\n",
    "All of these prevent overflow by using floating-point arithmetic.\n",
    "\n",
    "### Common Student Approaches\n",
    "\n",
    "**Approach 1:** Cast each channel\n",
    "```python\n",
    "gray = (r.astype(float) + g.astype(float) + b.astype(float)) / 3\n",
    "```\n",
    "\n",
    "**Approach 2:** Divide first (also works)\n",
    "```python\n",
    "gray = r/3.0 + g/3.0 + b/3.0\n",
    "```\n",
    "\n",
    "**Approach 3:** Cast the sum\n",
    "```python\n",
    "gray = (r + g + b).astype(float) / 3  # Still overflows before cast!\n",
    "```\n",
    "âš ï¸ This doesn't work â€” overflow happens before `.astype()`\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explanation (write your answer here):**\n",
    "\n",
    "*Why did the original code fail?*\n",
    "\n",
    "...\n",
    "\n",
    "*What did you change to fix it?*\n",
    "\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### OpenCV's Grayscale Conversion\n",
    "\n",
    "Even with the bug fixed, your result might look slightly different from OpenCV's conversion:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OpenCV's built-in grayscale conversion\n",
    "opencv_gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "cv_imshow(opencv_gray)\n",
    "\n",
    "print(\"Compare the two grayscale images carefully.\")\n",
    "print(\"Can you see the difference? It's subtle but present.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ğŸ”¬ Challenge 2 â€” OpenCV's Secret\n",
    "\n",
    "OpenCV doesn't simply average the three channels. It uses a **weighted** formula.\n",
    "\n",
    "**Your tasks:**\n",
    "1. Research what formula OpenCV uses for BGR â†’ grayscale\n",
    "2. Implement it below\n",
    "3. Explain **why** OpenCV uses this specific formula (hint: human vision)\n",
    "\n",
    "**Hint:** Search for \"ITU-R BT.601\" or look at OpenCV's color conversion documentation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# â”€â”€ Implement OpenCV's weighted grayscale conversion â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "gray = (r + g + b) / 3  # Replace with weighted formula\n",
    "# â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€\n",
    "\n",
    "cv_imshow(gray)\n",
    "\n",
    "# Compare to OpenCV's result\n",
    "difference = np.abs(gray.astype(float) - opencv_gray.astype(float))\n",
    "print(f\"\\nMax difference from OpenCV: {difference.max():.2f}\")\n",
    "print(f\"Mean difference: {difference.mean():.2f}\")\n",
    "print(\"\\nIf max difference < 1.0, your formula is correct!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## ğŸ”‘ SOLUTION â€” Challenge 2: Weighted Grayscale\n",
    "\n",
    "### The Answer\n",
    "\n",
    "OpenCV uses the **ITU-R BT.601** standard:\n",
    "\n",
    "```python\n",
    "gray = 0.114 * b + 0.587 * g + 0.299 * r\n",
    "```\n",
    "\n",
    "Or more explicitly:\n",
    "```python\n",
    "gray = (0.114 * b.astype(float) + \n",
    "        0.587 * g.astype(float) + \n",
    "        0.299 * r.astype(float))\n",
    "```\n",
    "\n",
    "### Why These Weights?\n",
    "\n",
    "Human eyes are **not equally sensitive to all colors:**\n",
    "\n",
    "| Color | Weight | Sensitivity |\n",
    "|-------|--------|-------------|\n",
    "| Red | 0.299 | Moderate |\n",
    "| **Green** | **0.587** | **Highest** |\n",
    "| Blue | 0.114 | Lowest |\n",
    "\n",
    "**Green dominates** because the human eye has more green-sensitive cone cells.\n",
    "\n",
    "### Historical Context\n",
    "\n",
    "This formula comes from analog television standards (NTSC, PAL). It ensures that grayscale TVs could display color broadcasts correctly.\n",
    "\n",
    "The weights are based on:\n",
    "1. **Photopic vision:** How bright humans perceive each color\n",
    "2. **CRT phosphor characteristics:** The actual colors of old TV screens\n",
    "\n",
    "### Modern Alternatives\n",
    "\n",
    "- **ITU-R BT.709** (HDTV): 0.0722*B + 0.7152*G + 0.2126*R\n",
    "- **sRGB luminance:** Similar to BT.709\n",
    "\n",
    "But BT.601 is still widely used for compatibility.\n",
    "\n",
    "### Teaching Moment\n",
    "\n",
    "This connects to several concepts:\n",
    "1. **Perception vs physics:** Light intensity â‰  perceived brightness\n",
    "2. **Standards:** Why everyone uses the same formula\n",
    "3. **Legacy:** Why old decisions persist (backward compatibility)\n",
    "\n",
    "Ask: \"What happens if you use equal weights (0.333 each)?\" \n",
    "Answer: Green areas look too bright, blue areas too dark in grayscale.\n",
    "\n",
    "### How to Check Your Answer\n",
    "\n",
    "```python\n",
    "# If your formula is correct, difference should be < 1.0\n",
    "difference = np.abs(gray - opencv_gray.astype(float))\n",
    "assert difference.max() < 1.0, \"Weights are incorrect\"\n",
    "```\n",
    "\n",
    "Small differences (< 1.0) are due to rounding in OpenCV's implementation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Explanation (write your answer here):**\n",
    "\n",
    "*What formula does OpenCV use?*\n",
    "\n",
    "...\n",
    "\n",
    "*Why this formula? (Hint: Which color are human eyes most sensitive to?)*\n",
    "\n",
    "..."
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 â€” INSTRUCTOR",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
