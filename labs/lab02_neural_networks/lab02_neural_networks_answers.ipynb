{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 2: The Very Basic Basics of Neural Networks\n",
    "\n",
    "## üéì SOLUTIONS\n",
    "\n",
    "**This notebook contains:**\n",
    "- ‚úÖ Complete solutions to all exercises\n",
    "- ‚úÖ Teaching notes and common student questions\n",
    "- ‚úÖ Expected results and benchmarks\n",
    "- ‚úÖ Additional experiments and extensions\n",
    "- ‚úÖ Grading rubrics and assessment criteria"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìã Teaching Notes\n",
    "\n",
    "### Learning Objectives\n",
    "1. Understand neural network structure (input, hidden, output layers)\n",
    "2. Learn PyTorch's nn.Module paradigm\n",
    "3. Understand training loop mechanics (forward, loss, backward, step)\n",
    "4. Appreciate the role of activation functions\n",
    "5. Develop intuition for hyperparameter selection\n",
    "\n",
    "### Time Allocation (90 min lab)\n",
    "- 0-10 min: Setup and imports\n",
    "- 10-30 min: Simple model walkthrough\n",
    "- 30-50 min: Improved models (hidden layer, ReLU)\n",
    "- 50-90 min: Student experimentation\n",
    "\n",
    "### Common Student Questions\n",
    "1. **\"Why zero_grad()?\"** ‚Üí Gradients accumulate by default in PyTorch\n",
    "2. **\"What's the difference between forward() and __call__()?\"** ‚Üí __call__ includes hooks\n",
    "3. **\"Why view(-1, 784)?\"** ‚Üí Reshapes to (batch_size, 784)\n",
    "4. **\"What's a good accuracy?\"** ‚Üí >95% is good for this lab\n",
    "5. **\"Why is my loss NaN?\"** ‚Üí Usually learning rate too high\n",
    "\n",
    "### Expected Student Performance\n",
    "- **A students:** Will achieve >97% accuracy, try multiple architectures, understand trade-offs\n",
    "- **B students:** Will achieve ~95% accuracy, complete all required exercises\n",
    "- **C students:** Will achieve ~92% accuracy, may struggle with experimentation\n",
    "- **Struggling students:** May not complete experimentation section, need help with PyTorch syntax"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Computer Vision Course - Lab 2: Neural Networks\n",
    "INSTRUCTOR VERSION\n",
    "\n",
    "This cell sets up the environment.\n",
    "Works automatically for both local and Google Colab!\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Detect environment\n",
    "IN_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"Computer Vision - Lab 2 Setup (INSTRUCTOR VERSION)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "if IN_COLAB:\n",
    "    print(\"\\nüîµ Running on Google Colab\")\n",
    "    print(\"-\" * 60)\n",
    "    \n",
    "    if not os.path.exists('computer-vision'):\n",
    "        print(\"üì• Cloning repository...\")\n",
    "        !git clone https://github.com/mjck/computer-vision.git\n",
    "        print(\"‚úì Repository cloned successfully\")\n",
    "    else:\n",
    "        print(\"‚úì Repository already exists\")\n",
    "    \n",
    "    %cd computer-vision/labs/lab02_neural_networks\n",
    "    print(f\"‚úì Current directory: {os.getcwd()}\")\n",
    "    \n",
    "    sys.path.insert(0, '/content/computer-vision')\n",
    "    print(\"‚úì Python path configured\")\n",
    "    \n",
    "    print(\"-\" * 60)\n",
    "    print(\"üü¢ Colab setup complete!\\n\")\n",
    "    \n",
    "else:\n",
    "    print(\"\\nüü¢ Running locally\")\n",
    "    print(\"-\" * 60)\n",
    "    print(f\"‚úì Current directory: {os.getcwd()}\")\n",
    "    \n",
    "    repo_root = os.path.abspath('../..')\n",
    "    if repo_root not in sys.path:\n",
    "        sys.path.insert(0, repo_root)\n",
    "    print(f\"‚úì Repository root: {repo_root}\")\n",
    "    \n",
    "    print(\"-\" * 60)\n",
    "    print(\"üü¢ Local setup complete!\\n\")\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"‚úÖ Environment ready!\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torchvision import datasets, transforms\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "\n",
    "# Import course utilities\n",
    "try:\n",
    "    from sdx import *\n",
    "    print(\"‚úì Imported course utilities (sdx module)\")\n",
    "except ImportError as e:\n",
    "    print(\"‚ùå Error importing sdx module\")\n",
    "    print(f\"   {e}\")\n",
    "    raise\n",
    "\n",
    "# Set device (GPU if available)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"‚úì Using device: {device}\")\n",
    "\n",
    "if device.type == 'cuda':\n",
    "    print(f\"  GPU: {torch.cuda.get_device_name(0)}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"‚úÖ All libraries imported successfully!\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading and Displaying the MNIST Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download and load MNIST dataset\n",
    "train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transforms.ToTensor())\n",
    "test_dataset = datasets.MNIST(root='./data', train=False, download=True, transform=transforms.ToTensor())\n",
    "\n",
    "# Extract images and labels as numpy arrays for visualization\n",
    "train_images = train_dataset.data.numpy()\n",
    "train_labels = train_dataset.targets.numpy()\n",
    "test_images = test_dataset.data.numpy()\n",
    "test_labels = test_dataset.targets.numpy()\n",
    "\n",
    "print(f\"Training set: {len(train_images)} images\")\n",
    "print(f\"Test set: {len(test_images)} images\")\n",
    "print(f\"Image shape: {train_images[0].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualizations\n",
    "cv_imshow(train_images[9])\n",
    "print(f\"Label: {train_labels[9]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grid view with labels\n",
    "cv_gridshow(train_images, start=10, stop=35, labels=train_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building Neural Networks\n",
    "\n",
    "### üéì Teaching Notes:\n",
    "- Emphasize that this first model has NO hidden layers\n",
    "- It's essentially logistic regression\n",
    "- Expected accuracy: ~92%\n",
    "- This establishes a baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Simple model: 784 ‚Üí 10 (no hidden layers)\n",
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.fc = nn.Linear(28 * 28, 10)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)  # Flatten\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "model = SimpleNN().to(device)\n",
    "print(model)\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f\"\\nTotal parameters: {total_params:,}\")\n",
    "print(f\"Expected: {(784 * 10 + 10):,}\")  # weights + biases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup training\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "# Prepare data loaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training function (reusable)\n",
    "def train_model(model, train_loader, criterion, optimizer, epochs=1):\n",
    "    model.train()\n",
    "    \n",
    "    for epoch in range(epochs):\n",
    "        running_loss = 0.0\n",
    "        correct = 0\n",
    "        total = 0\n",
    "        \n",
    "        pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{epochs}\")\n",
    "        \n",
    "        for images, labels in pbar:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.item()\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            \n",
    "            pbar.set_postfix({\n",
    "                'loss': f'{running_loss/(pbar.n+1):.4f}',\n",
    "                'acc': f'{100*correct/total:.2f}%'\n",
    "            })\n",
    "        \n",
    "        epoch_loss = running_loss / len(train_loader)\n",
    "        epoch_acc = 100 * correct / total\n",
    "        print(f\"Epoch {epoch+1}: Loss = {epoch_loss:.4f}, Accuracy = {epoch_acc:.2f}%\")\n",
    "\n",
    "def evaluate_model(model, test_loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    accuracy = 100 * correct / total\n",
    "    print(f\"Test Accuracy: {accuracy:.2f}%\")\n",
    "    return accuracy\n",
    "\n",
    "def plot_confusion_matrix(model, test_loader):\n",
    "    from sklearn.metrics import confusion_matrix\n",
    "    import seaborn as sns\n",
    "    \n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in test_loader:\n",
    "            images = images.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.numpy())\n",
    "    \n",
    "    cm = confusion_matrix(all_labels, all_preds)\n",
    "    \n",
    "    plt.figure(figsize=(10, 8))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "                xticklabels=range(10), yticklabels=range(10))\n",
    "    plt.xlabel('Predicted')\n",
    "    plt.ylabel('True')\n",
    "    plt.title('Confusion Matrix')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train simple model\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Training Simple Model (784 ‚Üí 10)\")\n",
    "print(\"Expected accuracy: ~92%\")\n",
    "print(\"=\" * 60 + \"\\n\")\n",
    "\n",
    "train_model(model, train_loader, criterion, optimizer, epochs=1)\n",
    "simple_acc = evaluate_model(model, test_loader)\n",
    "plot_confusion_matrix(model, test_loader)\n",
    "\n",
    "print(f\"\\nüìä Result: {simple_acc:.2f}% (Target: ~92%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Improved Model with Hidden Layer\n",
    "\n",
    "### üéì Teaching Notes:\n",
    "- Adding a hidden layer creates a true neural network\n",
    "- Without activation, multiple linear layers = one linear layer!\n",
    "- Expected accuracy: ~95% (without ReLU)\n",
    "- Students should notice modest improvement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImprovedNN(nn.Module):\n",
    "    def __init__(self, hidden_size=128):\n",
    "        super(ImprovedNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(28 * 28, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, 10)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "model = ImprovedNN(hidden_size=128).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "print(model)\n",
    "print(f\"\\nTotal parameters: {sum(p.numel() for p in model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Training Improved Model (784 ‚Üí 128 ‚Üí 10)\")\n",
    "print(\"Expected accuracy: ~94-95%\")\n",
    "print(\"=\" * 60 + \"\\n\")\n",
    "\n",
    "train_model(model, train_loader, criterion, optimizer, epochs=1)\n",
    "improved_acc = evaluate_model(model, test_loader)\n",
    "plot_confusion_matrix(model, test_loader)\n",
    "\n",
    "print(f\"\\nüìä Result: {improved_acc:.2f}% (Target: ~94-95%)\")\n",
    "print(f\"Improvement: +{improved_acc - simple_acc:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model with ReLU Activation\n",
    "\n",
    "### üéì Teaching Notes:\n",
    "- ReLU introduces non-linearity\n",
    "- Without it, stacked linear layers collapse to a single linear transformation\n",
    "- Expected accuracy: ~97%\n",
    "- **Key point:** This is where students see the real power of deep learning!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NNWithReLU(nn.Module):\n",
    "    def __init__(self, hidden_size=128):\n",
    "        super(NNWithReLU, self).__init__()\n",
    "        self.fc1 = nn.Linear(28 * 28, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(hidden_size, 10)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)  # Non-linearity!\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "model = NNWithReLU(hidden_size=128).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "print(model)\n",
    "print(f\"\\nTotal parameters: {sum(p.numel() for p in model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Training Model with ReLU (784 ‚Üí ReLU ‚Üí 128 ‚Üí 10)\")\n",
    "print(\"Expected accuracy: ~97%\")\n",
    "print(\"=\" * 60 + \"\\n\")\n",
    "\n",
    "train_model(model, train_loader, criterion, optimizer, epochs=1)\n",
    "relu_acc = evaluate_model(model, test_loader)\n",
    "plot_confusion_matrix(model, test_loader)\n",
    "\n",
    "print(f\"\\nüìä Result: {relu_acc:.2f}% (Target: ~97%)\")\n",
    "print(f\"Improvement over simple: +{relu_acc - simple_acc:.2f}%\")\n",
    "print(f\"Improvement over no activation: +{relu_acc - improved_acc:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä SOLUTION: Student Experimentation Section\n",
    "\n",
    "### Multiple Solution Examples\n",
    "\n",
    "Below are several example solutions showing different approaches students might take."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution 1: Deeper Network\n",
    "\n",
    "**Teaching Notes:**\n",
    "- Shows diminishing returns with more layers\n",
    "- Good students will try this\n",
    "- Expected: ~97-98%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeeperNN(nn.Module):\n",
    "    \"\"\"Solution 1: Deeper network with 3 hidden layers\"\"\"\n",
    "    def __init__(self):\n",
    "        super(DeeperNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(784, 256)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.fc3 = nn.Linear(128, 64)\n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.fc4 = nn.Linear(64, 10)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.relu1(self.fc1(x))\n",
    "        x = self.relu2(self.fc2(x))\n",
    "        x = self.relu3(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "        return x\n",
    "\n",
    "model = DeeperNN().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "print(\"Solution 1: Deeper Network (784‚Üí256‚Üí128‚Üí64‚Üí10)\")\n",
    "print(model)\n",
    "print(f\"Total parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "\n",
    "train_model(model, train_loader, criterion, optimizer, epochs=1)\n",
    "deeper_acc = evaluate_model(model, test_loader)\n",
    "print(f\"\\nüìä Accuracy: {deeper_acc:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution 2: Using Adam Optimizer\n",
    "\n",
    "**Teaching Notes:**\n",
    "- Adam often trains faster than SGD\n",
    "- Advanced students will discover this\n",
    "- Typically reaches higher accuracy in same number of epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdamNN(nn.Module):\n",
    "    \"\"\"Solution 2: Same architecture but with Adam optimizer\"\"\"\n",
    "    def __init__(self):\n",
    "        super(AdamNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(784, 128)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "model = AdamNN().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)  # Adam instead of SGD!\n",
    "\n",
    "print(\"Solution 2: Adam Optimizer\")\n",
    "print(\"Same architecture as before, but using Adam optimizer\")\n",
    "print(f\"Total parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "\n",
    "train_model(model, train_loader, criterion, optimizer, epochs=1)\n",
    "adam_acc = evaluate_model(model, test_loader)\n",
    "print(f\"\\nüìä Accuracy: {adam_acc:.2f}%\")\n",
    "print(f\"Comparison to SGD: {adam_acc - relu_acc:+.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution 3: Adding Dropout\n",
    "\n",
    "**Teaching Notes:**\n",
    "- Dropout helps prevent overfitting\n",
    "- Won't see huge benefit on MNIST (it's too easy)\n",
    "- Good to introduce the concept"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DropoutNN(nn.Module):\n",
    "    \"\"\"Solution 3: Adding dropout for regularization\"\"\"\n",
    "    def __init__(self):\n",
    "        super(DropoutNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(784, 256)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.dropout1 = nn.Dropout(0.2)\n",
    "        self.fc2 = nn.Linear(256, 128)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.dropout2 = nn.Dropout(0.2)\n",
    "        self.fc3 = nn.Linear(128, 10)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.dropout1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "\n",
    "model = DropoutNN().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "print(\"Solution 3: With Dropout (20%)\")\n",
    "print(model)\n",
    "print(f\"Total parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "\n",
    "train_model(model, train_loader, criterion, optimizer, epochs=1)\n",
    "dropout_acc = evaluate_model(model, test_loader)\n",
    "print(f\"\\nüìä Accuracy: {dropout_acc:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution 4: Training for Multiple Epochs\n",
    "\n",
    "**Teaching Notes:**\n",
    "- Simple but effective!\n",
    "- Shows the value of more training\n",
    "- Can reach 98%+ with enough epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NNWithReLU(hidden_size=128).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "print(\"Solution 4: Training for 5 Epochs\")\n",
    "print(\"Same architecture, just more training time\")\n",
    "\n",
    "train_model(model, train_loader, criterion, optimizer, epochs=5)\n",
    "multi_epoch_acc = evaluate_model(model, test_loader)\n",
    "plot_confusion_matrix(model, test_loader)\n",
    "\n",
    "print(f\"\\nüìä Accuracy after 5 epochs: {multi_epoch_acc:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution 5: Different Activation Functions\n",
    "\n",
    "**Teaching Notes:**\n",
    "- Comparison of ReLU, LeakyReLU, Tanh\n",
    "- ReLU usually wins for this problem\n",
    "- Good experiment for curious students"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FlexibleNN(nn.Module):\n",
    "    \"\"\"Solution 5: Flexible model to test different activations\"\"\"\n",
    "    def __init__(self, activation='relu'):\n",
    "        super(FlexibleNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(784, 128)\n",
    "        \n",
    "        if activation == 'relu':\n",
    "            self.activation = nn.ReLU()\n",
    "        elif activation == 'leaky_relu':\n",
    "            self.activation = nn.LeakyReLU()\n",
    "        elif activation == 'tanh':\n",
    "            self.activation = nn.Tanh()\n",
    "        elif activation == 'sigmoid':\n",
    "            self.activation = nn.Sigmoid()\n",
    "        \n",
    "        self.fc2 = nn.Linear(128, 10)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.activation(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x\n",
    "\n",
    "results = {}\n",
    "\n",
    "for activation_name in ['relu', 'leaky_relu', 'tanh', 'sigmoid']:\n",
    "    print(f\"\\nTesting {activation_name}...\")\n",
    "    model = FlexibleNN(activation=activation_name).to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "    \n",
    "    train_model(model, train_loader, criterion, optimizer, epochs=1)\n",
    "    acc = evaluate_model(model, test_loader)\n",
    "    results[activation_name] = acc\n",
    "\n",
    "print(\"\\n\" + \"=\" * 60)\n",
    "print(\"Activation Function Comparison:\")\n",
    "for name, acc in sorted(results.items(), key=lambda x: x[1], reverse=True):\n",
    "    print(f\"  {name:15s}: {acc:.2f}%\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solution 6: Best Performing Model\n",
    "\n",
    "**Teaching Notes:**\n",
    "- Combines best practices\n",
    "- Should achieve 98%+\n",
    "- This is what A students should aim for"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BestNN(nn.Module):\n",
    "    \"\"\"\n",
    "    Solution 6: Best performing model combining multiple techniques:\n",
    "    - Deeper architecture\n",
    "    - Batch normalization\n",
    "    - Dropout\n",
    "    - Adam optimizer\n",
    "    - Multiple epochs\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(BestNN, self).__init__()\n",
    "        self.fc1 = nn.Linear(784, 512)\n",
    "        self.bn1 = nn.BatchNorm1d(512)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.dropout1 = nn.Dropout(0.3)\n",
    "        \n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        self.bn2 = nn.BatchNorm1d(256)\n",
    "        self.relu2 = nn.ReLU()\n",
    "        self.dropout2 = nn.Dropout(0.3)\n",
    "        \n",
    "        self.fc3 = nn.Linear(256, 128)\n",
    "        self.bn3 = nn.BatchNorm1d(128)\n",
    "        self.relu3 = nn.ReLU()\n",
    "        self.dropout3 = nn.Dropout(0.2)\n",
    "        \n",
    "        self.fc4 = nn.Linear(128, 10)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)\n",
    "        \n",
    "        x = self.fc1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu1(x)\n",
    "        x = self.dropout1(x)\n",
    "        \n",
    "        x = self.fc2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.relu2(x)\n",
    "        x = self.dropout2(x)\n",
    "        \n",
    "        x = self.fc3(x)\n",
    "        x = self.bn3(x)\n",
    "        x = self.relu3(x)\n",
    "        x = self.dropout3(x)\n",
    "        \n",
    "        x = self.fc4(x)\n",
    "        return x\n",
    "\n",
    "model = BestNN().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "print(\"Solution 6: Best Performing Model\")\n",
    "print(\"Features: Deep architecture + BatchNorm + Dropout + Adam\")\n",
    "print(model)\n",
    "print(f\"\\nTotal parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
    "\n",
    "print(\"\\nTraining for 5 epochs...\")\n",
    "train_model(model, train_loader, criterion, optimizer, epochs=5)\n",
    "best_acc = evaluate_model(model, test_loader)\n",
    "plot_confusion_matrix(model, test_loader)\n",
    "\n",
    "print(f\"\\nüèÜ Best Model Accuracy: {best_acc:.2f}%\")\n",
    "print(f\"Improvement over baseline: +{best_acc - simple_acc:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìä Summary Comparison\n",
    "\n",
    "### All Results at a Glance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Compile all results\n",
    "results_summary = pd.DataFrame([\n",
    "    {'Model': 'Simple (784‚Üí10)', 'Parameters': '7,850', 'Epochs': 1, 'Optimizer': 'SGD', 'Accuracy': simple_acc},\n",
    "    {'Model': 'Hidden Layer (784‚Üí128‚Üí10)', 'Parameters': '101,770', 'Epochs': 1, 'Optimizer': 'SGD', 'Accuracy': improved_acc},\n",
    "    {'Model': 'With ReLU', 'Parameters': '101,770', 'Epochs': 1, 'Optimizer': 'SGD', 'Accuracy': relu_acc},\n",
    "    {'Model': 'Deeper Network', 'Parameters': '~235K', 'Epochs': 1, 'Optimizer': 'SGD', 'Accuracy': deeper_acc},\n",
    "    {'Model': 'Adam Optimizer', 'Parameters': '101,770', 'Epochs': 1, 'Optimizer': 'Adam', 'Accuracy': adam_acc},\n",
    "    {'Model': 'With Dropout', 'Parameters': '~235K', 'Epochs': 1, 'Optimizer': 'Adam', 'Accuracy': dropout_acc},\n",
    "    {'Model': 'Multi-Epoch (5)', 'Parameters': '101,770', 'Epochs': 5, 'Optimizer': 'Adam', 'Accuracy': multi_epoch_acc},\n",
    "    {'Model': 'Best Model', 'Parameters': '~657K', 'Epochs': 5, 'Optimizer': 'Adam', 'Accuracy': best_acc},\n",
    "])\n",
    "\n",
    "results_summary = results_summary.sort_values('Accuracy', ascending=False)\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"FINAL RESULTS SUMMARY\")\n",
    "print(\"=\" * 80)\n",
    "print(results_summary.to_string(index=False))\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Visualize\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.barh(results_summary['Model'], results_summary['Accuracy'])\n",
    "plt.xlabel('Test Accuracy (%)')\n",
    "plt.title('Model Comparison - MNIST Classification')\n",
    "plt.xlim(90, 100)\n",
    "for i, (model, acc) in enumerate(zip(results_summary['Model'], results_summary['Accuracy'])):\n",
    "    plt.text(acc, i, f' {acc:.2f}%', va='center')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéì Grading Rubric\n",
    "\n",
    "### Total: 100 points\n",
    "\n",
    "#### Completion (40 points)\n",
    "- All cells executed: 10 points\n",
    "- Simple model trained: 10 points\n",
    "- Hidden layer model trained: 10 points\n",
    "- ReLU model trained: 10 points\n",
    "\n",
    "#### Experimentation (30 points)\n",
    "- Tried at least 3 different architectures: 15 points\n",
    "- Documented what was tried: 10 points\n",
    "- Compared results: 5 points\n",
    "\n",
    "#### Analysis (20 points)\n",
    "- Explained observations: 10 points\n",
    "- Discussed what worked/didn't work: 10 points\n",
    "\n",
    "#### Performance (10 points)\n",
    "- >92% accuracy: 5 points (baseline)\n",
    "- >95% accuracy: 7 points (good)\n",
    "- >97% accuracy: 10 points (excellent)\n",
    "\n",
    "### Bonus (up to 10 points)\n",
    "- Achieved >98% accuracy: +5 points\n",
    "- Implemented novel architecture: +3 points\n",
    "- Created visualization/analysis: +2 points\n",
    "\n",
    "### Common Deductions\n",
    "- Didn't run all cells: -10 points\n",
    "- No experimentation section: -30 points\n",
    "- No analysis: -20 points\n",
    "- Plagiarism: 0 points + academic integrity violation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üîç Expected Student Mistakes\n",
    "\n",
    "### 1. Forgetting `model.train()` / `model.eval()`\n",
    "**Symptom:** Inconsistent results, dropout behaving oddly\n",
    "**Fix:** Always set mode before training/evaluation\n",
    "\n",
    "### 2. Not calling `optimizer.zero_grad()`\n",
    "**Symptom:** Loss doesn't decrease, gradients explode\n",
    "**Fix:** Clear gradients before each backward pass\n",
    "\n",
    "### 3. Wrong device placement\n",
    "**Symptom:** \"Expected all tensors to be on the same device\"\n",
    "**Fix:** Move both model and data to same device\n",
    "\n",
    "### 4. Learning rate too high\n",
    "**Symptom:** Loss becomes NaN, doesn't converge\n",
    "**Fix:** Reduce learning rate (try 0.001, 0.0001)\n",
    "\n",
    "### 5. No activation function\n",
    "**Symptom:** Adding layers doesn't help\n",
    "**Fix:** Add ReLU or other activation\n",
    "\n",
    "### 6. Wrong input shape\n",
    "**Symptom:** \"RuntimeError: mat1 and mat2 shapes cannot be multiplied\"\n",
    "**Fix:** Remember to flatten: `x.view(x.size(0), -1)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üìö Additional Resources for Students\n",
    "\n",
    "### For struggling students:\n",
    "- [PyTorch 60-Minute Blitz](https://pytorch.org/tutorials/beginner/deep_learning_60min_blitz.html)\n",
    "- [Neural Networks from Scratch](https://www.youtube.com/watch?v=aircAruvnKk)\n",
    "\n",
    "### For advanced students:\n",
    "- Try Fashion-MNIST dataset\n",
    "- Implement learning rate scheduling\n",
    "- Add data augmentation\n",
    "- Try different architectures (CNN preview)\n",
    "- Implement early stopping\n",
    "\n",
    "### Office Hours Topics:\n",
    "- Debugging PyTorch code\n",
    "- Understanding backpropagation\n",
    "- Choosing hyperparameters\n",
    "- Project ideas using neural networks"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
